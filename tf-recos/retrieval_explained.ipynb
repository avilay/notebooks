{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ratings = tfds.load(\"movie_lens/100k-ratings\", split=\"train\")\n",
    "full_movies = tfds.load(\"movie_lens/100k-movies\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "{'bucketized_user_age': <tf.Tensor: shape=(), dtype=float32, numpy=45.0>,\n",
      " 'movie_genres': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([7])>,\n",
      " 'movie_id': <tf.Tensor: shape=(), dtype=string, numpy=b'357'>,\n",
      " 'movie_title': <tf.Tensor: shape=(), dtype=string, numpy=b\"One Flew Over the Cuckoo's Nest (1975)\">,\n",
      " 'raw_user_age': <tf.Tensor: shape=(), dtype=float32, numpy=46.0>,\n",
      " 'timestamp': <tf.Tensor: shape=(), dtype=int64, numpy=879024327>,\n",
      " 'user_gender': <tf.Tensor: shape=(), dtype=bool, numpy=True>,\n",
      " 'user_id': <tf.Tensor: shape=(), dtype=string, numpy=b'138'>,\n",
      " 'user_occupation_label': <tf.Tensor: shape=(), dtype=int64, numpy=4>,\n",
      " 'user_occupation_text': <tf.Tensor: shape=(), dtype=string, numpy=b'doctor'>,\n",
      " 'user_rating': <tf.Tensor: shape=(), dtype=float32, numpy=4.0>,\n",
      " 'user_zip_code': <tf.Tensor: shape=(), dtype=string, numpy=b'53211'>}\n"
     ]
    }
   ],
   "source": [
    "print(len(full_ratings))\n",
    "pprint(next(iter(full_ratings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1682\n",
      "{'movie_genres': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([4])>,\n",
      " 'movie_id': <tf.Tensor: shape=(), dtype=string, numpy=b'1681'>,\n",
      " 'movie_title': <tf.Tensor: shape=(), dtype=string, numpy=b'You So Crazy (1994)'>}\n"
     ]
    }
   ],
   "source": [
    "print(len(full_movies))\n",
    "pprint(next(iter(full_movies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.MapDataset'> 100000\n",
      "{'movie_title': <tf.Tensor: shape=(), dtype=string, numpy=b\"One Flew Over the Cuckoo's Nest (1975)\">,\n",
      " 'user_id': <tf.Tensor: shape=(), dtype=string, numpy=b'138'>}\n"
     ]
    }
   ],
   "source": [
    "ratings = full_ratings.map(lambda x:{\n",
    "    \"movie_title\": x[\"movie_title\"],\n",
    "    \"user_id\": x[\"user_id\"]\n",
    "})\n",
    "print(type(ratings), len(ratings))\n",
    "pprint(next(iter(ratings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.MapDataset'> 1682\n",
      "<tf.Tensor: shape=(), dtype=string, numpy=b'You So Crazy (1994)'>\n"
     ]
    }
   ],
   "source": [
    "movie_titles = full_movies.map(lambda x: x[\"movie_title\"])\n",
    "print(type(movie_titles), len(movie_titles))\n",
    "pprint(next(iter(movie_titles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "shuffled_ratings = ratings.shuffle(100_000, seed=1, reshuffle_each_iteration=False)\n",
    "trainset = shuffled_ratings.take(80_000)\n",
    "testset = shuffled_ratings.skip(80_000).take(20_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sidebar: How does `batch` work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
      "array([b'You So Crazy (1994)', b'Love Is All There Is (1996)',\n",
      "       b'Fly Away Home (1996)'], dtype=object)>\n"
     ]
    }
   ],
   "source": [
    "# movies.batch(n) will return an iterator that will yield n movies at a time\n",
    "# until all moives have been iterated.\n",
    "pprint(next(iter(movie_titles.batch(3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: (1000,)\n",
      "batch 1: (682,)\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(movie_titles.batch(1000)):\n",
    "    print(f\"batch {i}: {batch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(1000,)\n",
      "(682,)\n"
     ]
    }
   ],
   "source": [
    "# just like with any iterator we can get all the batches with a call to `list`\n",
    "tp = list(movie_titles.batch(1000))\n",
    "print(len(tp))\n",
    "print(tp[0].shape)\n",
    "print(tp[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie_title': <tf.Tensor: shape=(3,), dtype=string, numpy=\n",
      "array([b\"One Flew Over the Cuckoo's Nest (1975)\",\n",
      "       b'Strictly Ballroom (1992)', b'Very Brady Sequel, A (1996)'],\n",
      "      dtype=object)>,\n",
      " 'user_id': <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'138', b'92', b'301'], dtype=object)>}\n"
     ]
    }
   ],
   "source": [
    "# Similarly ratings.batch will also return an iterator. However, because each element in ratings is\n",
    "# a dict, the batch method will yield a columnar dict with values as tensors. \n",
    "# In case of movie_titles, each element was a tensor, which is why the iterator yielded a row tensor.\n",
    "pprint(next(iter(ratings.batch(3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back to main tutorial\n",
    "There is no simple way to get all the movie titles in a single list. In PyTorch I'd have simply list'ed the entire `Dataset` and be done with it. But here I have to first create the batch iterator  (similar to PyTorch `Dataloader`), and then extract the data batch-by-batch, and finally concatenate all the batch tensors into one big tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (1682,)\n",
      "(1664,)\n"
     ]
    }
   ],
   "source": [
    "# Lets create the movie_title embedding table\n",
    "\n",
    "# Lets first get the list of all 1600 movie titles\n",
    "it = movie_titles.batch(1000)\n",
    "all_movie_titles = np.concatenate(list(it))\n",
    "print(type(all_movie_titles), all_movie_titles.shape)\n",
    "\n",
    "# and then get the unique movie titles from this ndarray\n",
    "uniq_movie_titles = np.unique(all_movie_titles)\n",
    "print(uniq_movie_titles.shape)\n",
    "\n",
    "movie_title_emb = tf.keras.layers.Embedding(len(uniq_movie_titles) + 1, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (100000,)\n",
      "(943,)\n"
     ]
    }
   ],
   "source": [
    "# Now lets create the user_id embedding table\n",
    "\n",
    "# get the list of all 100_000 user_ids in batches of 1000\n",
    "it = ratings.batch(1000).map(lambda x: x[\"user_id\"])\n",
    "all_user_ids = np.concatenate(list(it))\n",
    "print(type(all_user_ids), all_user_ids.shape)\n",
    "\n",
    "# and then get the unique user_ids from this ndarray\n",
    "uniq_user_ids = np.unique(all_user_ids)\n",
    "print(uniq_user_ids.shape)\n",
    "\n",
    "user_id_emb = tf.keras.layers.Embedding(len(uniq_user_ids) + 1, EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the two towers, the request tower (aka user tower) and the candidate tower (aka movie tower)\n",
    "\n",
    "# We need to map the string user_ids and movie_titles into their respective idx vals\n",
    "user_id2idx = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=uniq_user_ids, mask_token=None)\n",
    "movie_title2idx = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=uniq_movie_titles, mask_token=None)\n",
    "\n",
    "# And then pass the indexes through the embedding tables\n",
    "user_model = tf.keras.Sequential([user_id2idx, user_id_emb])\n",
    "movie_model = tf.keras.Sequential([movie_title2idx, movie_title_emb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems like FactorizedTopK needs all the candidate **embeddings**. \n",
    "# Remember this is not eager execution (unlike PT) so this compute graph is simply set up at this\n",
    "# time, there is no danger of getting untrained embeddings out just yet.\n",
    "metrics = tfrs.metrics.FactorizedTopK(candidates=movie_titles.batch(128).map(movie_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Approach\n",
    "\n",
    "The main approach is a bit unexpected. Lets say the input consists of a batch of 3 <user_id, movie_title> tuples.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\text{user}_1 & \\text{movie}_1 \\\\\n",
    "\\text{user}_2 & \\text{movie}_2 \\\\\n",
    "\\text{user}_3 & \\text{movie}_3 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The first step is to take all the users and get their embeddings from the `user_model`. Lets say our embedding dimension is 2 and the embedding of $\\text{user}_1$ is $u_1 = \\left[u_{11} \\; u_{12}\\right]$ and so on. Do the same for the movies.\n",
    "\n",
    "$$\n",
    "U = \\begin{bmatrix}\n",
    "u_{11} & u_{12} \\\\\n",
    "u_{21} & u_{22} \\\\\n",
    "u_{31} & u_{32} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "M = \\begin{bmatrix}\n",
    "m_{11} & m_{12} \\\\\n",
    "m_{21} & m_{22} \\\\\n",
    "m_{31} & m_{32} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "These two embeddings are passed to the `Retrieval` *task* (wonder why they decided to call this layer task?) which interacts the embeddings and calculates the softmax cross entropy loss. The loss is softmax cross entropy instead of binary cross entropy because this layer does something interesting. It uses the input batch of embeddings to create a bunch of negative samples like so.\n",
    "\n",
    "First it will calculate the dot product of each user with all the three movies, the one movie that the user has seen, and the other two that they haven't! This way we get 1 positive and 2 negative samples.\n",
    "\n",
    "$$\n",
    "U.M^T = \\begin{bmatrix}\n",
    "u_1.m_1 & u_1.m_2 & u_1.m_2 \\\\\n",
    "u_2.m_1 & u_2.m_2 & u_2.m_2 \\\\\n",
    "u_3.m_1 & u_3.m_2 & u_3.m_3 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then it will calculate the corresponding labels simply as an $I$.\n",
    "\n",
    "$$\n",
    "L = \\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now these two matrices can be thought of as the inputs and labels of a multi-class classification problem whose loss is a softmax loss. \n",
    "\n",
    "In this approach, it is possible that $u_1$ and $m_2$, which have been taken as a negative sample, could actually be a positive sample outside of this particular mini-batch. The `Retrieval` class addresses this by zero-ing out the logits of such pairs. At first glance this might seem like a no-op because softmax does not take into account zeros, it only takes the logit corresponding to the 1 in the one-hot label. And the label for $u1^Tm_2$ is $0$, so how does changing its logit matter? However, by zero-ing out the logit of an element in the row gives all the other elements a greater share of the probability distribution thereby increasing the probability of the $u_1^Tm_1$. This is needed because the model might have learnt to give greater dot products for $u_1^Tm_2$ based on other mini-batches. And because of the high value of that element in the logits for this mini-batch, the probability associated with $u_1^Tm_1$, the positive sample in this batch will have a lower probability and a higher cost. This will prevent the model from learning to give a high dot product for these embeddings. The `Retrieval` class takes in a tensor of `candidate_ids` to address the accidental negative hits. However, the exact structure of this tensor is not well documented. I was not able to reverse-engineer its structure even after looking at the source code.\n",
    "\n",
    "The metrics are calculated based on straight up positive samples. **TODO:** Dig into the metrics calculation code to figure out exactly what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = tfrs.tasks.Retrieval(metrics=metrics)\n",
    "\n",
    "# tfrs has provided a convenience module that calls the forward method using gradient tape.\n",
    "# This is a very simple model that just takes the \n",
    "class MovielensModel(tfrs.Model):\n",
    "    def __init__(self, user_model, movie_model):\n",
    "        super().__init__()\n",
    "        self.movie_model = movie_model\n",
    "        self.user_model = user_model\n",
    "        self.task = task\n",
    "        \n",
    "    def compute_loss(self, features, training=False):\n",
    "        user_embeddings = self.user_model(features[\"user_id\"])\n",
    "        positive_movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
    "        return self.task(user_embeddings, positive_movie_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MovielensModel(user_model, movie_model)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "10/10 [==============================] - 4s 404ms/step - factorized_top_k: 0.0455 - factorized_top_k/top_1_categorical_accuracy: 2.1250e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0033 - factorized_top_k/top_10_categorical_accuracy: 0.0090 - factorized_top_k/top_50_categorical_accuracy: 0.0703 - factorized_top_k/top_100_categorical_accuracy: 0.1444 - loss: 69861.8509 - regularization_loss: 0.0000e+00 - total_loss: 69861.8509\n",
      "Epoch 2/3\n",
      "10/10 [==============================] - 4s 401ms/step - factorized_top_k: 0.0966 - factorized_top_k/top_1_categorical_accuracy: 0.0021 - factorized_top_k/top_5_categorical_accuracy: 0.0151 - factorized_top_k/top_10_categorical_accuracy: 0.0322 - factorized_top_k/top_50_categorical_accuracy: 0.1556 - factorized_top_k/top_100_categorical_accuracy: 0.2780 - loss: 67514.2237 - regularization_loss: 0.0000e+00 - total_loss: 67514.2237\n",
      "Epoch 3/3\n",
      "10/10 [==============================] - 4s 401ms/step - factorized_top_k: 0.1111 - factorized_top_k/top_1_categorical_accuracy: 0.0025 - factorized_top_k/top_5_categorical_accuracy: 0.0206 - factorized_top_k/top_10_categorical_accuracy: 0.0419 - factorized_top_k/top_50_categorical_accuracy: 0.1810 - factorized_top_k/top_100_categorical_accuracy: 0.3094 - loss: 66327.8033 - regularization_loss: 0.0000e+00 - total_loss: 66327.8033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f84280da240>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader = trainset.shuffle(100_000).batch(8192).cache()\n",
    "model.fit(trainloader, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving\n",
    "\n",
    "The user and movie models have been trained in such a way that the embeddings they output will have a larger dot product if the user is likely to watch the movie. If $u$ is the user embedding and $m$ is the movie embedding, then the larger $u^Tm$ is, the more likely the user is going to watch the movie. This property can be used during serving time to make retrieval fast. \n",
    "\n",
    "At system startup time, all the movie embeddings are calculated. Then, when a request comes in, the user embedding of the requesting user is calculated. Now a simple nearest neighbor algorithm can be used to find the top $k$ movies whose dot product with this user is the highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = AnnoyIndex(EMBEDDING_DIM, \"dot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None,) for input Tensor(\"string_lookup_1_input:0\", shape=(None,), dtype=string), but it was called on an input with incompatible shape ().\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None,) for input Tensor(\"string_lookup_1_input:0\", shape=(None,), dtype=string), but it was called on an input with incompatible shape ().\n"
     ]
    }
   ],
   "source": [
    "# here we give each movie an \"id\" which just its index value in the dataset\n",
    "movie_embeddings = movie_titles.enumerate().map(lambda idx, title: (idx, title, model.movie_model(title)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_id_to_title = dict((idx, title) for idx, title, _ in movie_embeddings.as_numpy_iterator())\n",
    "\n",
    "# We unbatch the dataset because Annoy accepts only scalar (id, embedding) pairs.\n",
    "for movie_id, _, movie_embedding in movie_embeddings.as_numpy_iterator():\n",
    "    index.add_item(movie_id, movie_embedding)\n",
    "\n",
    "# Build a 10-tree ANN index.\n",
    "index.build(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates: [b\"Kid in King Arthur's Court, A (1995)\", b'Now and Then (1995)', b'Kazaam (1996)'].\n",
      "Candidates: [b'Hot Shots! Part Deux (1993)', b'Heavy Metal (1981)', b'Nightmare Before Christmas, The (1993)'].\n",
      "Candidates: [b'Once Upon a Time in the West (1969)', b'Spellbound (1945)', b'Notorious (1946)'].\n"
     ]
    }
   ],
   "source": [
    "for row in testset.batch(1).take(3):\n",
    "    query_embedding = model.user_model(row[\"user_id\"])[0]\n",
    "    candidates = index.get_nns_by_vector(query_embedding, 3)\n",
    "    print(f\"Candidates: {[movie_id_to_title[x] for x in candidates]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
